# Diachronic

## Experiments:

### bert-base-cased-swedish-historical

- 4 gpus
- batch_size 8
- lr: 5e-5
- epochs: 3

#### eval

- perplexity plain bert (kb) - 6.7
- perplexity: 3.6

---

## TODO

- preprocess diachronic again and evaluate --> publish to huggingface
- train model on new published dataset.
